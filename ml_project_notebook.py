# -*- coding: utf-8 -*-
"""ML Project notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7I2GCxGYXpdO6l2DcM7MpUtY8UCNJnZ

# Data pre-processing

Import the necessary libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import time
# %matplotlib inline

"""Download the dataset and read data into a pandas dataframe. Code taken from the following [page](https://nijianmo.github.io/amazon/index.html#subsets)."""

# !wget --no-check-certificate https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Cell_Phones_and_Accessories_5.json.gz

import gzip
import json


def parse(path):
    g = gzip.open(path, "rb")
    print("loading", end="\r", flush=True)
    for l in g:
        yield json.loads(l)


def getDF(path):
    i = 0
    df = {}
    for d in parse(path):
        df[i] = d
        i += 1
    return pd.DataFrame.from_dict(df, orient="index")

getDfStart = time.time()
df = getDF("Cell_Phones_and_Accessories_5.json.gz")
print("Dataframe loaded in", time.time() - getDfStart, "seconds")

"""Create a new column in the data frame containing the categories of the reviews. The ratings of 4 and 5 have a `positive` category, 1 and 2 have a `negative` category and 3 is marked as `neutral`. Then convert the target value from strings to categorical data."""


def categorize_rating(rating):
    if rating >= 4:
        return "positive"
    elif rating <= 2:
        return "negative"
    else:
        return "neutral"


df["category"] = df["overall"].apply(categorize_rating)
df["category"] = df["category"].astype("category")

# **Preprocessing Step 1**: Cretae a new data frame using only the `category` column and the concatenation of `reviewText` and `summary` columns.

data = df.assign(reviewText=df["reviewText"] + " " + df["summary"])[
    ["category", "reviewText"]
]

# **Preprocessing Step 2**: Remove rows with null values.

data = data.dropna()

"""**Preprocessing Step 3:** Convert all characters to lowercase."""

data["reviewText"] = data["reviewText"].str.lower()

"""**Preprocessing Step 4:** Remove any HTML tags and URLs if present, as well as extra white spaces."""

import re


def clean_text(text):
    text = re.sub(r"<[^>]+>|http\S+", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


data["reviewText"] = data["reviewText"].apply(clean_text)

"""**Preprocessing Step _:** Perform spelling corrections (may be unnecessary, takes a very long time)."""

# !!sudo apt-get install swig3.0
# !sudo pip install jamspell
# !wget https://github.com/bakwc/JamSpell-models/raw/master/en.tar.gz
# !tar -xvf en.tar.gz

import jamspell
jsp = jamspell.TSpellCorrector()
assert jsp.LoadLangModel("en.bin")
i = 0

def correct_spelling(text):
    global i
    i += 1
    print(i, end="\r", flush=True)
    return jsp.FixFragment(text)

correct_spelling_start = time.time()
data["reviewText"] = data["reviewText"].apply(correct_spelling)
print("Spelling corrected in", time.time() - correct_spelling_start, "seconds")

data.to_csv("data.csv", index=False)
print("Saved to csv")

"""**Preprocessing Step 5:** Tokenization"""

# import nltk
# from nltk.tokenize import word_tokenize
# from nltk.tokenize import sent_tokenize
#
# nltk.download('punkt')
#
# data['wordTokens'] = data['reviewText'].apply(word_tokenize)
#
# data['sentTokens'] = data['reviewText'].apply(sent_tokenize)

"""**Preprocessing Step 6:** Remove stop words."""

# from nltk.corpus import stopwords
#
# nltk.download('stopwords')
# stop_words = set(stopwords.words('english'))
#
# def rm_stop_words(words):
#  return [word for word in words if word not in stop_words]
#
# data['wordTokens'] = data['wordTokens'].apply(rm_stop_words)
#
# def rm_sent_stop_words(sentences):
#   filtered_sentences = []
#
#   for sentence in sentences:
#     word_tokens = word_tokenize(sentence)
#     filtered_sentences.append(' '.join([word for word in word_tokens if word not in stop_words]))
#
#   return filtered_sentences
#
# data['sentTokens'] = data['sentTokens'].apply(rm_sent_stop_words)

"""**Preprocessing Step 7:** Remove punctuation (except for `!` and `?`), special characters and numbers."""

# def clean_tokens(tokens):
#   return [re.sub(r'[^a-zA-Z\s!?-]', '', token) for token in tokens]
#
# data['wordTokens'] = data['wordTokens'].apply(clean_tokens)
#
# data['sentTokens'] = data['sentTokens'].apply(clean_tokens)

"""**Preprocessing Step 8:** Lemmatization (reduce words to their base or root form, for instance 'crying' into 'cry')."""

# import spacy
#
# nlp = spacy.load("en_core_web_sm")
#
# # Function to lemmatize a list of word tokens
# def lemmatize_tokens(tokens):
#     # Create a spaCy Doc from the tokens (rejoin them into a string for spaCy processing)
#     doc = nlp(' '.join(tokens))
#     # Return the lemmatized tokens
#     return [token.lemma_ for token in doc]
#
# with pd.option_context('display.max_colwidth', None):
#     print(data.iloc[5000])

# CNN

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPool2D, Dropout, Flatten
#
# model = Sequential()
#
# from tensorflow.keras.optimizers import Adam
#
# optimizer = Adam(learning_rate=0.001)
# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
#
# # Train the model, iterating on the data in batches of 32 samples
# model.fit(x_train, y_train, epochs=15, batch_size=32, validation_split=1/6)
